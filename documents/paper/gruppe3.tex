% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage[ngerman]{babel}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{algorithm2e}
\usepackage{floatflt}
\RestyleAlgo{ruled}
\begin{document}
	%
	\title{Gruppe 3: Mehrschichtige und dezentrale Entscheidungsprozesse in Agentensystemen}
	\pagenumbering {gobble}
	%
	\titlerunning{Mehrschichtige und dezentrale Entscheidungsprozesse in Agentensystemen}
	% If the paper title is too long for the running head, you can set
	% an abbreviated paper title here
	%
	\author{Heinz Stadler\and
		Melinda Betz\and
		Phil Heger\and
		Björn Wladasch}
	%
	\authorrunning{H. Stadler et al.}
	% First names are abbreviated in the running head.
	% If there are more than two authors, 'et al.' is used.
	%
	\institute{FernUniversität in Hagen, Universitätsstraße 47, 58097 Hagen, Deutschland
		\email{\{vorname.nachname\}@studium.fernuni-hagen.de}\\
		\url{https://www.fernuni-hagen.de}}
	%
	\maketitle              % typeset the header of the contribution
	%
	%
	\section{Einleitung}
	Der MAPC \textit{(Multi-Agent Programming Contest)} 2021 \cite{MAPC2021} bildet die thematische Grundlage für die in diesem Dokument beschrieben Agentensysteme. Es handelt sich um zwei Varianten, basierend auf der BDI-Architektur \cite{Bratman1987}, die das menschliche Denken und Schlussfolgern abstrahiert bzw. nachbildet. Beide Konzepte, nachfolgend als Agentensystem V1 (siehe Kap. \ref{agentV1}) bzw. V2 (siehe Kap. \ref{agentV2}) bezeichnet, unterscheiden sich konzeptionell durch unterschiedlich stark zentralisierte Entscheidungsprozesse.
	Ziel des dualen Ansatzes soll sein, die Leistungsfähigkeit beider Varianten zu prüfen und verschiedene Lösungsansätze zu erhalten, die zwischen den Systemen ausgetauscht werden können.

	
	Als Grundlage dient das \textit{javaagents} Gerüst der MASSim (\textit{Multi-Agent Systems Simulation Platform}) \cite{EISMASSim} in der Sprache Java. Die Entscheidung für eine universelle Programmiersprache basiert auf dem Wunsch nach umfangreichen Werkzeugen und Bibliotheken zur Verifikation und Problemfindung und wird durch die Aussage von T. Albrecht untermauert, dass für viele Teilnehmer am MAPC die Fehlerfindung eine schwierige und zeitintensive Aufgabe darstellt (vgl. \cite[S. 17]{Ahlbrecht2021}).
	
	Die Konzeption der Agentensysteme erfolgte mittels UML-Diagrammen in gemeinsamer Gruppendiskussion.
	Der Entwicklungsprozess wurde über das Versionsverwaltungssystem \textit{Github} und dessen Aufgaben- bzw. Fehlermanagement gesteuert. Es wurden Aufgaben ermittelt, Bearbeiter zugewiesen und für jede Aufgabe einzelne \textit{Feature-Branches} erstellt. Vor der Integration in den Hauptstamm des Quellcodes wurden die Änderungen durch ein weiteres Gruppenmitglied überprüft und anschließend freigegeben. Diese Vorgehensweise soll die Qualität des Quelltextes erhöhen, sowie das gemeinsame Verständnis über diesen fördern.
	
	
	\section{Agentensystem V1 - Gruppenbeitrag Heinz Stadler}\label{agentV1}
	Die Analyse der Ergebnisse des \textit{15. Multi-Agent Programming Contest} \cite{Ahlbrecht2021} ergibt, dass nicht nur die Entscheidungsfindung der Agenten eine Herausforderung darstellt, sondern ebenso der Aufbau einer konsistenten und umfangreichen Wissensbasis (vgl. \cite[S. 29]{AhlbrechtFitBut2021}) sowie die effiziente Problemfindung (vgl. \cite[S. 17]{Ahlbrecht2021}). \\
	Aus dieser Erkenntnis erfolgte der Aufbau und die Verifikation einer Wissensverwaltung (siehe \ref{wissensverwaltung}), die sowohl eine Datenstruktur zur Speicherung der Simulationsinformationen, als auch eine Lösung zum Aufbau einer globalen Karte des Simulationsgebiets umfasst. Zusätzlich wurde an der Konzeptionierung und Implementierung des Agentensystems V1 (siehe \ref{archAgentV1}) und dessen Ziel- und Absichtsfindung, sowie der Entwicklung einer intelligenten Wegfindung (siehe \ref{wegfindung}) gearbeitet. Zur effektiven Verifizierung der Ergebnisse wurde ein grafisches Analysewerkzeug (siehe \ref{verifikation}) erstellt.
	
	\subsection{Architektur}\label{archAgentV1}
	Das Agentensystem V1 erweitert das BDI-Konzept \cite{Bratman1987} um zusätzliche Daten-, Berechnungs- und Entscheidungsebenen, die in Abb. \ref{g3:architecture} illustriert sind. Die zusätzlichen Entscheidungsebenen erweitern den BDI-Agenten zu einer bidirektionalen, vertikalen Schichtarchitektur \cite[S. 61-62]{Weiss2000}, auf die in Abschnitt \ref{absichtsfindung} näher eingegangen wird.
	
	
	\begin{wrapfigure}[13]{r}{0.4\linewidth}
		\vspace{-8mm}
		\includegraphics[scale=0.7]{./Referenzen/Architekturdiagramm.pdf}
		\caption{Architektur Agent V1}
		\label{g3:architecture}
		
	\end{wrapfigure}
	
	Jeder Agent wurde mit einer Vorgesetzteninstanz, die eine zusätzliche Entscheidungsebene bildet, kombiniert und in einem Thread parallelisiert. Werden Agenten zu einer Gruppe zusammengeführt, bleibt eine Vorgesetzteninstanz aktiv. Alle übrigen Vorgesetzteninstanzen der Gruppe werden passiv und übernehmen nur noch die Weiterleitung von Nachrichten an die aktive Entität.
	
	Die Kommunikation zwischen den Instanzen in verschiedenen Threads erfolgt über Nachrichten, die in einer threadsicheren Warteschlange zwischengespeichert werden. Die Verständigung des Agenten mit seinem direkten Vorgesetzten wird mittels Methodenaufrufen realisiert.
	
	Die einzelnen Agentengruppen aktualisieren jeweils eine Karte mit im Simulationsverlauf erhaltenen Umgebungsinformationen. Die Karten werden zusammen mit dem Modul zur Wegfindung von einem zentralem, threadsicheren Navigationsmodul verwaltet, das als Einzelstück ausgeführt ist. 
	
	
	\subsection{Wissensverwaltung}\label{wissensverwaltung}
	Jeder Agent hat Zugriff auf eine individuelle Wissensbasis (\textit{Beliefs}), die von der Simulation bereitgestellte Informationen auswertet und speichert.
	Die aus dem Sichtfeld des Agenten erhaltenen partiellen Umgebungsdaten werden an das Navigationsmodul weitergeleitet und in einer chronologisch fortgeschriebenen Karte zusammengeführt.
	
	Beim Simulationsstart erhält jeder Agent eine Karte mit festgelegter Initialgröße (Abb. \ref{Karte} Abschn. 1), die beim Erkunden der Umgebung erweitert wird (Abb. \ref{Karte} Abschn. 2). Treffen sich zwei Agenten aus unterschiedlichen Gruppen, wird dies vom Navigationsmodul erkannt und an die aktiven Vorgesetzten beider Gruppen gemeldet. Stimmen beide Vorgesetzte einer Vereinigung zu, werden ihre Karten überlagert (Abb. \ref{Karte} Abschn. 3) und schließlich zusammengeführt (Abb. \ref{Karte} Abschn. 4). Die so neu entstandene Karte ermöglicht im weiteren Simulationsverlauf die aktuelle Position aller Agenten einer Gruppe untereinander zu bestimmen. Aus diesen Informationen kann durch jeweils zwei Agenten, die sich in entgegengesetzte Richtungen bewegen und sich durch die periodische Randbedingung \cite{Bungartz2013} des Simulationsgebiets zwangsläufig wieder treffen, die Kartengröße ermittelt werden. Nach erfolgreicher Ermittlung wird die Karte beschnitten, wobei die Informationen abgeschnittener Bereiche auf der gegenüberliegenden Seite eingefügt werden und dadurch nicht verloren gehen (Abb. \ref{Karte} Abschn. 5).
	\vspace{-3mm}
	\begin{figure}[h]
		\center	
		\includegraphics[scale=0.76]{./Referenzen/Kartenmerge.pdf}
		\caption{Die Karte im Simulationsverlauf}
		\label{Karte}
	\end{figure}
	
	\vspace{-8mm}
	\subsection{Wegfindung}\label{wegfindung}
	\begin{wrapfigure}[13]{r}{0.23\linewidth}
		\vspace{-8mm}
		\includegraphics{./Referenzen/Pathfinding.pdf}
		\caption{Datenstruktur Wegfindung (aus realer Simulation)}
		\label{pathfinding}
	\end{wrapfigure}
	Unter dem Begriff Wegfindung verstehen wir die Berechnung des Abstands zwischen zwei Punkten mit Berücksichtigung von Hindernissen im Simulationsgebiet, sowie das Finden eines kürzesten Wegs zwischen diesen Punkten.
	


	Zur Lösung der Aufgaben wurden klassische Suchalgorithmen wie z.B. A* \cite{Hart1968} als auch dynamische Echtzeitalgorithmen \cite[S. 182-191]{WeissYokoo2000} untersucht. Da dynamische Algorithmen nicht zur Entfernungsermittlung eingesetzt werden können, wurde der A* Algorithmus mit Heuristik Manhattan-Distanz \cite{Craw2017} angewendet.
	
	Durch 50-100 Berechnungen pro Agent und Simulationsschritt scheidet eine CPU \textit{(Central Processing Unit)} basierte Lösung aufgrund Zeitbeschränkungen in der Absichtsfindung aus. Daher erfolgte die Implementierung des Suchalgorithmus als \textit{Computeshader} in der GLSL (\textit{OpenGL Shading Language}) \cite{GLSL}, der auf der GPU \textit{(Graphics Processing Unit)} ausgeführt wird. Mit dieser Technologie kann die Wegfindung auf über 1000 Berechnungen pro Simulationsschritt in praktikabler Berechnungszeit\footnote{ca. 20-250 ms auf einer integrierten Intel UHD Graphics 620 Grafikkarte} parallelisiert werden.
	
	Die Ein- und Ausgabedatenstruktur bildet eine 3D-Textur (siehe Abb. \ref{pathfinding}), die zusätzlich als visuelle Hilfestellung bei der Problemfindung dient und sowohl die Eingabekarte codiert als auch die Berechnungsergebnisse aufnimmt.
	
	Bei der Wegberechnung wird das Überqueren von mit Hindernissen belegten Zellen mit zusätzlichen Kosten bewertet. Die Agenten erhalten somit einen kürzesten Weg, der sowohl Wege durch Hindernisse, als auch Wege um diese herum enthalten kann.
	
	\subsection{Ziel- und Absichtsfindung}\label{absichtsfindung}
	Die Ziel- und Absichtsfindung erfolgt über zwei Ebenen in einer vertikalen, bidirektionalen Schichtarchitektur bestehend aus einem BDI-Agenten und dessen Vorgesetzteninstanz (Abb. \ref{desires}). Der Agent ist in der Lage, für sich individuelle Ziele zu entwickeln. Die Vorgesetzteninstanz kümmert sich um die Koordinierung der Agenten zur Bewältigung ihrer Gruppenziele. Die Kommunikation erfolgt sowohl vom Agenten zur Vorgesetzeninstanz als auch in entgegengesetzter Richtung.
	
	
	\begin{figure}[h]
		\vspace{-3mm}
		\centering
		\includegraphics[scale=0.7]{./Referenzen/Entscheidungsfindung2.pdf}
		\caption{Diagramm Ziel- und Absichtsfindung Agent V1}
		\label{desires}
	\end{figure}
	\vspace{-3mm}
	
	Zu Beginn jedes Simulationsschritts werden die aktuellen Simulationsdaten ausgewertet und die Wissensbasis der Agenten aktualisiert. Umgebungsinformationen werden an das Navigationsmodul weitergeleitet, um Zielpunkte für die Wegführung zu generieren. Zeitgleich senden die Agenten einen Ausschnitt ihres aktuellen Zustands an ihre Vorgesetzteninstanzen. Diese versuchen aus allen Agenten in ihrer Gruppe effektive Kombinationen zu bilden, um Mehrblockaufgaben zu konstruieren, die Kartengröße zu erkunden oder die Bewachung einer Zielzone zu koordinieren.
	
	Nach Abschluss der Wegfindung wird die Wissensbasis der Agenten aktualisiert. Die Agenten ergänzen ihre Ziele durch Optionen, die durch neue Einblockaufgaben entstanden sind, sowie durch Optionen die von ihren aktiven Vorgesetzteninstanzen übermittelt wurden. Nicht mehr erfüllbare Optionen oder bereits erfüllte Gruppenoptionen werden anschließend aus den Zielen gelöscht.
	
	Jede Option besitzt eine Priorität, die im Fall von Einblock-Aufgaben auch dynamisch sein kann. Durch den modularen Aufbau der Optionen, die auch Unteroptionen enthalten können, ist es möglich Teilfunktionalitäten wiederzuverwenden und die Absichtsfindung zu vereinfachen. Nach einer Sortierung gemäß Priorität wird iterativ nach ausführbaren und noch nicht erfüllten Zielen gesucht, um diese in eine neue Absicht umzuwandeln. Aus der Absicht wird schließlich die nächste Agentenaktion extrahiert und an die Simulation übermittelt.
	
	Alle bearbeiteten Optionen (über 30 Stück) sind im Paket \textit{desires} der Implementierung enthalten. Eine detaillierte Auflistung und Beschreibung überschreitet den Rahmen dieser Arbeit.
	
	
	\subsection{Verifikation und Problemfindung}\label{verifikation}
	Die Validierung verschiedener Strategien erfolgte über die erreichte Punktzahl in Testspielen und lieferte zufriedenstellende Ergebnisse. Die Methoden zur Verwaltung der Karte wurden mittels JUnit Tests \cite{JUnit} verifiziert. Im Gegensatz dazu konnte das Verhalten der Agenten nicht effizient durch Einzeltests verifiziert werden, da der Entscheidungsprozess in großer Abhängigkeit mit der dynamischen Wissensbasis der Agenten steht.
	Als Teststrategie wurde stattdessen das genaue Beobachten der Agenten gewählt, analog eines Trainers einer Sportmannschaft. Wurden Probleme evident, erfolgten gezielte Einzeltests um diese zu beheben.
	
	Dieser Ansatz erforderte zusätzliche visuelle Information, die der Monitor, über den die Simulation beobachtet werden kann, nicht liefert. Aus diesem Grund wurde ein grafisches Analysewerkzeug (siehe Abb. \ref{debugger}) implementiert, das einen detaillierten Einblick über den aktuellen Entscheidungsprozess und die Wissensbasis der Agenten liefert. Neben allgemeiner Informationen über den selektierten Agenten werden Information über Gruppen- und Individualziele sowie Simulationsinformationen und Ergebnisse der Wegfindung angezeigt.
	
	
	\begin{figure}
		\vspace{-3mm}
		\includegraphics[scale=0.091]{./Referenzen/Debugger3.png}
		\caption{Ausschnitt aus grafischem Analysewerkzeug}
		\label{debugger}
		\vspace{-3mm}
	\end{figure}
	
	Das Analysewerkzeug hat sich als sehr wertvoll erwiesen und ermöglichte mithilfe dynamischer Echtzeitinformationen, die effiziente Weiterentwicklung der Agenten.
	
	
	\section{Agent V2 - Gruppenbeitrag Melinda Betz}\label{agentV2}
	Neben meiner Tätigkeit als Gruppenlead habe ich einen alternativen Agenten V2 entwickelt.
	
	\subsection{ Architektur}
	Der Agent V2 arbeitet mit der Step-Methode, analog dem BasicAgent aus Massim. 
	Der Agent verwendet nicht das komplette Pathfinding des V1, da OpenGL auf meinen AMD Rechnern nicht, zumindest nicht performant, funktionierte. Der Agent stellt also seine eigenen Berechnungen an, um zu einem Ziel zu kommen. Außerdem nutzt er eigene Desires, nicht die des Agenten V1. Die Desires sind dabei aufteilbar in solche, die keine Task benötigen und Desires zur Bearbeitung einer Task. Bei Letzteren ist ein Teil nur für Mehr-Block-Tasks zuständig.
	\\
	\\
	\begin{tabular}{lll}
	\textbf{ohne Task} & \textbf{mit Task} & \textbf{Mehr\-Block\-Task}\\
	LocalExploreDesire & GoAbandonedBlockDesire & MasterMultiBlocksDesire\\
	GoAdoptRoleDesire & GoDispenserDesire & HelperMultiBlocksDesire\\
	ExploreMapSizeDesire & GoGoalZoneDesire & Helper2MultiBlocksDesire\\
	& SubmitDesire & ConnectMultiBlocksDesire\\
	\end{tabular}
	
	\subsection{Entscheidungsfindung}
	Ein Agent durchläuft in jedem Step alle Desires und prüft ob sie in seinem momentanen Zustand (Belief) möglich bzw. ausführbar sind. Für alle ausführbaren Desires wird dynamisch, abhängig auch vom Arbeitsstand des Desires, eine Priorität vergeben. Das Desire mit der höchsten Priorität wird dann zur Intention welche in diesem Step vom Agenten ausgeführt wird.
	
	\subsection{Strategie und Task Bearbeitung}
	Wenn zwei Agenten sich treffen, schließen sie sich zu Supervisor-Gruppen zusammen, um ihr Sichtfeld zu vergrößern. Die kleinere Gruppe wird dabei in die größere integriert. Jeder Agent holt sich zuerst die Rolle Worker. Diese Rolle kann alles, manches zwar nicht ganz optimal, aber gut genug.
	
	Alle Worker ohne Block besorgen sich selbständig einen Block. Um eine möglichst breite Auswahl an Blöcken zu erreichen, darf die maximale Anzahl eines Blocktyps (Setup Variable) nicht überschritten werden. Mit diesem Block bewegen sie sich in Richtung Goal Zone. Jeder Agent in einer Goal Zone, der gerade nicht an einer Mehr-Block-Task arbeitet (AgentCooperations), prüft in jedem Step für alle aktiven Tasks, ob er den innersten Block dieser Task besitzt. Eine Ein-Block-Task kann so direkt bearbeitet werden (Block in Position bringen und submitten). Für eine Mehr-Block-Task benötigt der Agent noch mindestens einen Helper, der einen weiteren passenden Block beisteuern kann, um eine neue Adhoc-Gruppe (AgentCooperation) mit sich als Master bilden zu können. In der Klasse AgentsCooperations wird festgehalten wer gerade in welcher Rolle (Master, Helper) mit welcher Task beschäftigt ist. Für alle Agenten ist dort auch der Stand der Taskabarbeitung aller anderen Beteiligten ersichtlich. Es existiert im Grunde keine zentrale Stelle zur Koordination der Agenten, jedoch gibt es einstellbare Regeln (Setup Variablen) wie viele Mehr-Block-Tasks gleichzeitig möglich sein sollen etc.. Das soll vor Klumpenbildung in der Goal Zone schützen und parallel weiterhin Ein-Block-Tasks ermöglichen.
	
	\subsection{Umgebungsfindung und Synchronisation}
	Ein Agent V2 kennt alles (vor allem Dispenser und GoalZones), was schon einmal in seinem Sichtfeld oder dem eines Agenten seiner Supervisor-Gruppe war. Entfernung und Richtung zu diesen Punkten kann er selbst ermitteln.
	
	Die Synchronisation von Agenten ist, neben der beschriebenen Bearbeitung von Mehr-Block-Tasks, wichtig für die Bestimmung der Mapgröße. Dabei wird für die Ermittlung von Höhe und Breite der Map, jeweils aus den ersten beiden Treffen zweier Agenten, zentral abgewickelt über die Klasse AgentMeetings, eine AgentCooperation gebildet. Der Master läuft einmal um die Map herum. Der Helper wartet bis dieser wieder zurück ist. Aus der Position des Masters bei dem erneuten Treffen kann nun die genaue Größe (Höhe oder Breite) der Map berechnet werden. 
	
	\subsection{Schwierigkeiten und Lösungsstrategien}
	Eine Hauptschwierigkeit war, nicht genau zu wissen, was ein Agent gerade attached hat, da diese Information im Percept des Servers nicht detailliert enthalten ist. Die Lösung, sich diese Information in Variablen zu merken, funktionierte zwar prinzipiell, stieß aber spätestens bei Einführung der Clear-Events an Grenzen. Außerdem hat man Schwierigkeiten, wenn ein Agent mit einer Geschwindigkeit größer zwei läuft und in einem Schritt abbricht (Returncode partial\_fail). Es ist dann nicht möglich herauszufinden, wie weit der Agent vor dem fail gekommen ist, wo er sich also gerade auf dem Spielfeld befindet. Hier ist die etwas unbefriedigende, aber stets erfolgreiche Lösung die: kein Agent läuft schneller als zwei (was durch die Rolle Worker automatisch gegeben ist). 
	
	\subsection{Verbesserungspotential}
	Was ich noch deutlich verbessern müsste ist, wie oben schon erwähnt, zum einen die dynamischere Erkennung der Umgebung (Goal Zones, Dispenser etc.) sowie das Nachführen der Informationen über die aktuellen Attachements der Agenten. Beides ist durch meine Versuche, diese Neuerungen für Turnier 6 einzubauen, eigentlich nur schlimmer geworden und war in „älteren Agenten“ schon mal wesentlich besser.
	
	Potential würde ich auch noch bei meinen oben beschriebenen AgentCooperations sehen. Diese sind derzeit, einmal angelegt, zu starr. Der Master wartet auf seine Helper geduldig bis zum Ende aller Tage, genau genommen bis zur Deadline der betroffenen Task. Eine dynamischere Variante, bei der der Master unterdessen neue Gelegenheiten mit anderen Agenten wahrnehmen kann, wäre wohl zu bevorzugen.  
	
	\section{Stören des Gegners - Gruppenbeitrag Phil Heger}
	Zu Beginn der Gruppenarbeit wurde neben einem eigenen Logging-Modul eine gemeinsam in der Gruppe anhand von UML-Diagrammen entwickelte Architektur für den Umgang mit den \textit{Desires} prototypisch umgesetzt. Die Umsetzung stellte sich jedoch als nicht praktikabel heraus und musste in der Folge überarbeitet werden. 
	
	Auf den Agenten-Architekturen aufbauend wurden anschließend Strategien zum Stören gegnerischer Agenten entwickelt.
	
	\subsection{Logging}
	Anfangs wurde ein Logging-Modul (\textit{AgentLogger.java}) implementiert, in dem der Logging-Output konfiguriert wird. Dieses Modul ermöglicht es, den Logging-Output, der von einem Agenten in einem bestimmten \textit{Desire} oder Modul ausgegeben wird, in eine eigene Datei zu schreiben. Dadurch sind die Entscheidungsfindung dieses Agenten und seine Zustandsänderungen besser nachvollziehbar, als wenn der gesamte Logging-Output aller Agenten in der gleichen Konsole bzw. Datei ausgegeben wird und nachträglich gefiltert werden muss. 
	
	\subsection{Strategien zum Stören gegnerischer Agenten}
	Durch eine gezielte \textit{clear}-Aktion auf die Position des Gegners wird dessen Energielevel um einen definierten Betrag verringert. Beträgt sein Energielevel 0, dann wird der Agent für einige Runden deaktiviert und verliert die Verbindung zu allen mit ihm verbunden Blöcken. Die Wirksamkeit einer einfachen Strategie (z.\,B. Angreifen beliebiger Agenten an beliebigen Stellen der Karte) wird durch folgende Aspekte stark verringert:
	\begin{itemize}
		\item{Bei der clear-Aktion muss das Feld angegeben werden, auf dem sich der gegnerische Agent am Ende der Runde befinden wird. Diese Position ist bei gegnerischen Agenten jedoch unbekannt, da er sich in alle Richtungen bewegen oder auch stehenbleiben kann. Die Wahrscheinlichkeit, den Gegner zu treffen, ist dadurch sehr gering.}
		\item{Der Schaden einer erfolgreichen \textit{clear}-Aktion ist nicht sehr groß. So beträgt der max. mögliche Schaden, wenn sich der angegriffene Agent in einem angrenzenden Feld befindet, nur 16\,Punkte bei einer Gesamtenergie von 100\,Punkten. Der Schaden halbiert sich mit der Distanz zum gegnerischen Agenten. Hinzu kommt, dass die Erfolgswahrscheinlichkeit der \textit{clear}-Aktion bei allen Rollen (außer der Rolle \textit{digger}) nur 30\,\% beträgt.}
	\end{itemize}
	Für ein wirksames Stören der Gegner ist daher eine komplexere Strategie und das Annehmen der Rolle \textit{digger} erforderlich.
	
	\subsubsection{Dispenser blockieren}
	Die erste Idee besteht darin, das Sammeln von Blöcken für die gegnerischen Agenten zu erschweren, indem ein eigener Agent auf das gleiche Feld wie ein \textit{Dispenser} geht und dort verbleibt. Dadurch ist es nicht mehr möglich an diesem \textit{Dispenser} neue Blöcke zu erzeugen. Dabei werden \textit{Dispenser} ausgewählt, die für die Bearbeitung der aktuellen Aufgaben am wichtigsten sind. Aufgrund der hohen Anzahl an \textit{Dispensern} in den Turnier-Konfigurationen zeigte sich jedoch, dass dieser Ansatz nicht praktikabel ist, da eine große Anzahl an eigenen Agenten für das Blockieren der vielen \textit{Dispenser} notwendig wäre. Um die eigenen Agenten nicht ebenfalls zu behindern, müssen diese über blockierte \textit{Dispenser} informiert werden.
	
	\subsubsection{Goal Zone verteidigen}
	Bei diesem Ansatz wurde der in der Einleitung dieses Unterkapitels erwähnte grundlegende Mechanismus (\textit{clear}-Aktion auf Position des gegnerischen Agenten) umgesetzt, wobei der eigene Agent gegnerische Agenten nur in der Zielzone angreift, da die gegnerischen Agenten hier oft auf andere Agenten warten und sich in dieser Zeit nicht bewegen. Der eigene Agent kann sich dadurch nähern und mit dem größtmöglichen Schaden angreifen. Ist der gegnerische Agent deaktiviert, werden alle mit ihm verbundenen Blöcke gelöst. Diese können anschließend vom angreifenden Agenten mit \textit{clear}-Aktionen entfernt werden. Neben dem Deaktivieren des Agenten für einige Runden war auch der Aufwand des Holens der Blöcke für umsonst. Hinzu kommt die Chance, den gegnerischen Algorithmus des Zusammenbauens von größeren Aufgaben zu stören.
	
	Für die Umsetzung der Strategie wurden folgende Teilaufgaben gelöst:
	\begin{itemize}
		\item{Zuordnung eines Agenten zu jeder Zielzone}
		\item{Analyse von Dingen, die mit einem Gegner verbunden sind bzw. sein könnten (direkt an ihn angrenzende, zusammenhängende Blöcke)}
		\item{Auswahl eines zu attackierenden Gegners basierend auf den mit ihm verbundenen Dingen und der Distanz zu ihm}
		\item{Bewegungen in der Zielzone (um Hindernisse herum, auf den Gegner zu bzw. patrouillieren, wenn sich keine geeigneten Gegner im Sichtfeld befinden)}
		\item{Energie des angegriffenen Gegners mitzählen}
		\item{Gegnerverfolgung (um die Energie eines Agenten mitzählen zu können und für die Verfolgung und Fortsetzung des Angriffs, wenn er sich bewegt)}
		\item{Zuletzt angegriffenen Gegner merken und nicht direkt noch einmal angreifen}
	\end{itemize}
	
	Getestet wurde die Strategie, indem die eigenen Agenten auch als Gegnerteam eingesetzt wurden. Die Strategie ist nur wirksam, wenn sich gegnerische, Blöcke tragende Agenten ausreichend lange in der Zielzone nicht bewegen. Gegen 1-Block-Aufgaben ist die Strategie dadurch völlig wirkungslos, da die gegnerischen Agenten dabei in die Zielzone laufen und sofort die Aufgabe abgeben.
	
	\section{Gruppenbeitrag Björn Wladasch - Kartengröße bestimmen}
	Nach der Einarbeitung in die Strukturen der MASSim (\textit{Multi-Agent Systems Simulation Platform}) \cite{EISMASSim} sollte eine Möglichkeit implementiert werden, die tatsächliche Größe der Karte zu bestimmen.
	Dadurch, dass sich die Karte zu allen Seiten endlos wiederholt, scheint sie aus Sicht der Agenten unendlich sein. Für eine effiziente Pfadfindung und eine günstige Koordinierung der Agenten, ist  die tatsächliche Größe der Karte von erheblichem Vorteil. Im folgenden wird beschrieben, welche Ansätze verfolgt wurden, um die Kartengröße mit dem Agenten V1 zu bestimmen.
	
	\subsection{Kartengröße berechnen}
	Die grundsätzliche Idee Agenten in entgegengesetzte Richtungen zu schicken und zu beobachten, wann Sie vor dem jeweilig anderen Agenten wieder auftauchen, stammt aus der Turnierbeschreibung des letzten Wettbewerbs \cite[S.136]{MAPC2021}.
	
	Um die Kartenbreite ( bzw. -höhe) zu bestimmen, wurde der Abstand der Agenten zueinander sowohl beim Start der Vermessung als auch beim erneuten Aufeinandertreffen gemessen und die Anzahl der Schritte in die jeweilige Richtung gespeichert. Aus diesen Werten lässt sich - im Idealfall - die Breite bzw. Höhe der Karte ermitteln.
	
	Bei der Umsetzung traten allerdings verschiedene Probleme auf, welche nur zum Teil gelöst werden konnten.
	
	\subsection{Koordination von Agenten}
	Für die Bestimmung der Kartengröße wurden fünf Agenten zu einer Gruppe zusammengefasst. Ein Agent (als Supervisor) koordinierte die anderen Agenten und gab das Ergebniss der Vermessung weiter. Die anderen vier Agenten wurden jeweils in eine der Himmelsrichtungen entsendet.
	Bei der Umsetzung stellte sich aber heraus, dass auf großen Karten mit relativ wenigen Agenten schon das Zusammenstellen der Gruppe lange dauert und somit viel Zeit verloren ging, manchmal konnte nicht einmal eine Gruppe gebildet werden.
	
	Zudem mussten die Agenten auf Ihrem Weg andern Agenten und Hindernissen ausweichen und wurden dadurch von ihrem Weg abgelenkt. Dies führte dazu, dass sich die Agenten nicht wieder begegneten,
	weil sie sich auf der zu ihrer Laufrichtung orthogonalen Achse zu weit von einander entfernt hatten. Dies wurde durch die Festlegung einer gemeinsame Basislinie (also die x- bzw. y-Achse auf der sich die Agenten bewegten) behoben, zu der sich die Agenten nach einem Ausweichmanöver wieder hin orientieren mussten.
	
	\subsection{Kommunikation in der Agentengruppe}
	Anfänglich hat jeder Agent der Gruppe alle vor ihm auftauchenden Agenten des eigenen Teams mit von allen Agenten des Gruppe auf eine Übereinstimmung mit seiner relativen Position überprüfen lassen. Dieses Vorgehen führte aber zu einem erheblichen Kommunikationsaufwand sowie zu falsch-positiven Sichtungen, da verschiedene Agenten zufällig einen anderen Agenten aus Ihrem Team an der selben relativen Position gesehen haben.
	
	Daher wurden die Agenten einer Achse miteinander bekannt gemacht. Agenten die gemeinsam eine Achse vermessen sollten, kannten den Namen des jeweils anderen Agenten und konnten so jeweils direkt miteinander kommunizieren, was sowohl den Kommunikationsaufwand erheblich gesenkt als  auch die falsch-positiven Sichtungen beseitigt hat.
	
	Da die beiden Probleme der Berechnung der Kartengröße aus Abständen und Schrittzählern sowie die  Koordination von 5 Agenten in einer Gruppe nicht zu befriedigenden Ergebnissen geführt haben, wurde durch die Gruppe eine alternative Lösung implementiert, welche dann im letzten Turnier eingesetzt wurde.
	
	\section{Turniere}\label{Turniere}
	Die Gruppe nahm an den Turnieren 2-6 teil. Den Großteil der Spiele bestritt das Agentensystem V1. Das System V2 übernahm jeweils ein Spiel in den Turnieren 2-5. 
	
	\subsection{Agent V1}
	Das Agentensystem zeigte sich über alle teilgenommenen Turniere hinweg wettbewerbsfähig. Probleme in einzelnen Begegnungen wurden detailliert dokumentiert und im weiteren Entwicklungsverlauf behoben oder zumindest entschärft. Die Folgenden Absätze geben ein Überblick über die Leistung und Probleme in den einzelnen Turnieren.
	
	\subsubsection{Turnier 2}
	Es war erfreulich, dass 10 Agenten in der Lage waren bis zu 370 Punkte über Einblock-Aufgaben zu erringen. Trotzdem zeigte sich noch großes Verbesserungspotential bei der Befreiung von Agenten aus festgefahrenen Situationen sowie dem Ausweichen von Agenten untereinander.
	
	\subsubsection{Turnier 3}
	Die erreichte Punktzahl über Einblock-Aufgaben konnte auf bis zu 720 gesteigert werden. Ausschlaggebend dafür waren bessere Strategien zur Befreiung der Agenten aus festgefahrenen Situationen. Trotzdem waren weiterhin teilweise übermäßige Gruppenbildung und die daraus resultierende gegenseitige Behinderung der Agenten zu beobachten.
	
	\subsubsection{Turnier 4}
	Die Agenten waren erstmals in der Lage Mehrblock-Aufgaben zu bearbeiten. Das Zusammenspiel der Agenten war noch nicht zufriedenstellend, wodurch sich ein Rückgang der erreichten Maximalpunkte auf 680 ergab. Positiv war zu beobachten, dass die Gruppenbildung und die daraus resultierende gegenseitige Behinderung im Vergleich zum Vorturnier abgenommen hat.
	
	\subsubsection{Turnier 5}
	Die Behebung eines Fehlers im Entscheidungsprozess erhöhte die Leistungsfähigkeit der Agenten und sicherte der Gruppe den Turniersieg mit maximal 1300 erreichten Punkten. In diesem Turnier wurde erstmalig in einzelnen Begegnungen aggressive Strategien gegen die gegnerischen Agenten eingesetzt.
	
	\subsubsection{Turnier 6}
	Das Turnier 6 bot neue Herausforderungen, da die Kartengröße nicht bekannt war, die Zielzonen sich bewegten und Meteoriteneinschläge auf der Karte ergänzt wurden. Die Agenten bewältigten die neuen Herausforderungen zufriedenstellend und konnten auch dieses Mal den Turniersieg mit einer Maximalpunktzahl von 910 sichern. Die Schwierigkeitserhöhung war deutlich in der erreichten Durchschnittspunktzahl sichtbar. Trotzdem konnten die Agenten in einzelnen Spielen die erhöhte Schwierigkeit durch die Abgabe von Dreiblock-Aufgaben kompensieren.
	
	Interessant war zu beobachten, dass speziell die Gruppe 1 sehr aggressiv gegen die Agenten vorging und dadurch eine spannende Begegnung entstand. Das Abwehren solchen Verhaltens wäre eine potenzielle Verbesserungsmöglichkeit für das Agentensystem. 
	
	\subsubsection{Bonusspiel - Jeder gegen Jeden}
	Zum Abschluss wurde ein Spiel mit 6 x 25 Agenten durchgeführt. Trotz der Vielzahl an gegnerischen und eigenen Entitäten zeigten sich die implementierten Strategien erfolgreich. Es wurden 1370 Punkte erreicht und das Spiel trotz erhöhter Gruppenbildung und gegenseitiger Störung mit deutlichem Abstand gewonnen.
	Positiv war zu beobachten, dass die 25 Agenten weiterhin performant arbeiteten und in sämtlichen Schritten alle Instanzen eine Aktion an den Simulationsserver übermittelten.
	
	\subsection{Agent V2}
	 Nachdem der Agent V2 seine Einsätze bei den ersten Turnieren überaus erfolgreich bestritten hatte, war er bei Turnier 6 nicht am Start, da er sowohl Probleme mit dem Erkennen der wechselnden Goal Zones hatte, vor allem mit dem Wegfall von Goal Zones, als auch mit dem Erkennen des Verlustes eines Blockes durch einen Clear-Event und so weniger Tasks als bisher erfolgreich bearbeiten konnte. Im Nachhinein wäre diese Vorsicht jedoch nicht nötig gewesen, da der Leistungsabfall zwar gegenüber dem zu diesem Zeitpunkt genialen Agenten V1 deutlich zu erkennen war, die anderen Gruppen aber anscheinend ebenfalls und mindestens so stark mit diesen neu hinzugekommen Problemen zu kämpfen hatten.
	
	\section{Rekapitulation und Ausblick}
	Die Umsetzung des erarbeiteten Wissens lieferte Lösungen, die speziell in den letzten beiden Turnieren sehr erfolgreich waren (siehe Kapitel \ref{Turniere}).
	
	
	Die Entscheidung zwei Architekturansätze zu verfolgen muss kritisch bewertet werden. Beide Ansätze behinderten sich zwar nicht, es stellten sich jedoch kaum Synergieeffekt ein. Die entwickelten Ziele des Agentensystem V2 wurden auf dessen internen Aufbau hin optimiert und waren somit nicht für das Agentensystem V1 verwendbar. Folglich blieb der erhoffte Austausch verschiedener Lösungsansätze unter den Systemen aus.
	
	Sowohl der mehrschichtige Entscheidungsprozess des Agentensystems V1, als auch der vollständig dezentrale Ansatz des Systems V2, lieferten funktionierende, kompetitive Lösungen. Das Abschneiden in den Turnieren legt nahe, dass der mehrschichtige Ansatz Vorteile gegenüber der dezentralen Lösung birgt. Dies bleibt aber eine Vermutung, da die unterschiedlichen Entscheidungsprozesse in eigene Architekturen eingebunden sind, die den direkten Vergleich erschweren.
	
	Die Gruppe ist mit der erreichten Funktionalität und deren Qualität zufrieden. Dennoch besteht vielfältiges Verbesserungspotential in der Entscheidungs- und Strategiefindung der Agenten.
	
	%
	% ---- Bibliography ----
	%
	% BibTeX users should specify bibliography style 'splncs04'.
	% References will then be sorted and formatted in the correct style.
	%
	% \bibliographystyle{splncs04}
	% \bibliography{mybibliography}
	%
	\newpage\begin{thebibliography}{8}
		\bibitem{Ahlbrecht2021}
		Ahlbrecht, T., Dix, J., Fiekas. N. und T. Krausburg: The 15th Multi-Agent Programming Contest, in Ahlbrecht, T., Dix, J., Fiekas. N. und T. Krausburg (Hrsg.): The Multi-Agent Programming Contest 2021, Springer, Heidelberg, 2021
		\bibitem{AhlbrechtFitBut2021}
		Uhlir, V., Zboril, F., Vidensky, F.: FIT BUT: Rational Agents in the Multi-Agent Programming Contest, in Ahlbrecht, T., Dix, J., Fiekas, N., Krausburg, T. (Hrsg.): The Multi-Agent Programming Contest 2021, Springer, Heidelberg, 2021
		\bibitem{Hart1968}
		Hart, P. E., Nilsson, N. J. und Raphael, B.: A Formal Basis for the Heuristic Determination of Minimum Cost Paths, in IEEE Transactions on Systems Science and Cybernetics, 4. Auflage, Nummer 2, Seiten 100-107, Juli 1968
		\bibitem{Weiss2000}
		Wooldridge, M.: Intelligent Agents, in Weiss, G. (Hrsg.): Multiagent Systems, 2. Auflage, The MIT Press, Cambridge, 2000
		\bibitem{WeissYokoo2000}
		Yokoo, M., Ishida, T.: Search Algorithms for Agents, in Weiss, G. (Hrsg.): Multiagent Systems, 2. Auflage, The MIT Press, Cambridge, 2000
		\bibitem{Bratman1987}
		Bratman, M.: Intention, plans, and practical reason, Harvard University Press, Cambridge, 1987
		\bibitem{GLSL}
		Kessenich, J., Baldwin, D., Rost, R.: The OpenGL® Shading Language, Version 4.60.7, https://registry.khronos.org/OpenGL/specs/gl/GLSLangSpec.4.60.pdf, abgerufen am 10.09.2022
		\bibitem{MAPC2021}
		Amaral, C. J., et al.: JaCaMo Builders: Team Description for the Multi-agent Programming Contest 2020/21, in  Ahlbrecht, T., Dix, J., Fiekas, N., Krausburg T. (Hrsg.): The Multi-Agent Programming Contest 2021, Springer, Heidelberg, 2021, Seite 136
		\bibitem{Bungartz2013}
		Bungartz, H.J., Zimmer, S., Buchholz, M., Pflüger, D.: Modellbildung und Simulation Eine anwendungsorientierte Einführung, 2. Auflage, Springer Spektrum, Berlin Heidelberg, 2013
		\bibitem{JUnit}
		JUnit 5, https://junit.org/junit5, abgerufen am 11.09.2022
		\bibitem{Craw2017}
		Craw, S.: Manhattan Distance in Encyclopedia of Machine Learning and Data Mining, Springer, Bosten, 2017, Seite 790-791
		\bibitem{EISMASSim}
		EISMASSim Documentation, \\ https://github.com/agentcontest/massim\_2022/blob/main/docs/eismassim.md, abgerufen am 21.08.2022
	\end{thebibliography}
\end{document}